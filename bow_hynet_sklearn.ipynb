{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T01:03:06.737249Z",
     "start_time": "2024-12-10T01:03:06.734990Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from kornia.feature import HyNet, extract_patches_from_pyramid\n",
    "from sklearn.cluster import KMeans"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:01:16.994674Z",
     "start_time": "2024-12-10T23:01:16.984586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loadImagePathsAndLAFs(baseDir, prefix, type):\n",
    "    # Paths for the pickle files created during sift feature calculations\n",
    "    imagePathsFile = os.path.join(baseDir, f\"{prefix}_{type}_image_paths.pkl\")\n",
    "    LAFsFile = os.path.join(baseDir, f\"{prefix}_{type}_sift_keypoint_LAFs.pkl\")\n",
    "    \n",
    "    # Initialize variables\n",
    "    imagePaths = None\n",
    "    lafs = None\n",
    "    \n",
    "    # Load from files\n",
    "    with open(imagePathsFile, 'rb') as f:\n",
    "        imagePaths = pickle.load(f)\n",
    "        \n",
    "    with open(LAFsFile, 'rb') as f:\n",
    "        lafs = pickle.load(f)\n",
    "\n",
    "    return imagePaths, lafs"
   ],
   "id": "454f05880c490a74",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:01:29.431083Z",
     "start_time": "2024-12-10T23:01:29.426330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extractHyNetDescriptors(imagePaths, lafs, hynet):\n",
    "    # Initialize empty list\n",
    "    descriptors = []\n",
    "    \n",
    "    # For each image\n",
    "    for i in range(len(imagePaths)):\n",
    "        # Read image in grayscale\n",
    "        image = cv2.imread(imagePaths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Convert to torch tensor and bring to kornia format (normalized float values)\n",
    "        imageTensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(dtype=torch.float32) / 255\n",
    "        \n",
    "        # Do the same for laf\n",
    "        lafTensor = torch.from_numpy(lafs[i]).unsqueeze(0).to(dtype=torch.float32)\n",
    "        \n",
    "        # Get patches of the image\n",
    "        imagePatches = extract_patches_from_pyramid(img=imageTensor, laf=lafTensor, PS=32, normalize_lafs_before_extraction=False)\n",
    "        # Reshape to the hynet input format\n",
    "        imagePatches = imagePatches.squeeze(0)\n",
    "        \n",
    "        # Get HyNet descriptors from patches\n",
    "        imageDescriptors = hynet(imagePatches)\n",
    "\n",
    "        # Convert to numpy array and add to list\n",
    "        descriptors.append(imageDescriptors.detach().cpu().numpy())\n",
    "\n",
    "    return descriptors"
   ],
   "id": "7b1104ca419fc8d6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:01:37.677251Z",
     "start_time": "2024-12-10T23:01:37.671714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def performKMeans(trainedModels, K, descriptors, clusterLabels, type):\n",
    "\n",
    "    if type == \"train\":\n",
    "        # Stack all features from train images into a single input matrix\n",
    "        X = np.vstack(descriptors)\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = KMeans(n_clusters=K, random_state=537, verbose=True)\n",
    "        model.fit(X)\n",
    "        \n",
    "        # Add the computed cluster labels for the current K\n",
    "        clusterLabels[K] = model.labels_\n",
    "        \n",
    "        # Add model to trained models\n",
    "        trainedModels[K] = model\n",
    "        \n",
    "    else:\n",
    "        # Stack features of test images\n",
    "        y = np.vstack(descriptors)\n",
    "        \n",
    "        # Get trained model\n",
    "        model = trainedModels[K]\n",
    "        \n",
    "        # Predict labels for test images, and add to dict for current K\n",
    "        labels = model.predict(y)\n",
    "        clusterLabels[K] = labels"
   ],
   "id": "6314f357994dd809",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:01:57.100394Z",
     "start_time": "2024-12-10T23:01:57.096393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def computeHistograms(K, descriptors, clusterLabels, histograms):\n",
    "    # Get cluster labels of all the features (for the current K = number of bins in histogram)\n",
    "    allLabels = clusterLabels[K]\n",
    "    \n",
    "    # Initialize an empty array to hold histograms of every image in the set\n",
    "    allHistograms = []\n",
    "    \n",
    "    # Last index checked in labels, start with 0\n",
    "    lastIndex = 0\n",
    "    \n",
    "    # For every image, get image's descriptor\n",
    "    for descriptor in descriptors:\n",
    "        # Get number of features in the HyNet descriptor of the image (=number of labels for the image)\n",
    "        labelCount = descriptor.shape[0]\n",
    "        # Index to check until for the image, in all labels\n",
    "        nextIndex = lastIndex + labelCount\n",
    "        # Get cluster labels for the current image\n",
    "        labels = allLabels[lastIndex:nextIndex]\n",
    "        # Initialize histogram for the image with current number of bins\n",
    "        histogram = np.zeros(K)\n",
    "        \n",
    "        # For each cluster label\n",
    "        for label in labels:\n",
    "            # Increase the number of words/features in current bag/bin by 1\n",
    "            histogram[label] += 1\n",
    "            \n",
    "        # Normalize histogram (make sum of elements = 1)\n",
    "        histogram = histogram / labelCount\n",
    "        # Add to list\n",
    "        allHistograms.append(histogram)\n",
    "        \n",
    "        # Update last index\n",
    "        lastIndex = nextIndex\n",
    "        \n",
    "    \n",
    "    # Save all histograms for the current K\n",
    "    histograms[K] = allHistograms"
   ],
   "id": "ac0f4fa917ed6951",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:02:08.262442Z",
     "start_time": "2024-12-10T23:02:08.257932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exportToFiles(descriptors, histograms, prefix, type, dataDir):\n",
    "    \n",
    "    fileNameOfHyNetDescriptors = f\"{prefix}_{type}_hynet_descriptors.pkl\"\n",
    "    filePathOfHyNetDescriptors = os.path.join(dataDir, fileNameOfHyNetDescriptors)\n",
    "    with open(filePathOfHyNetDescriptors, 'wb') as f:\n",
    "        pickle.dump(descriptors, f)\n",
    "\n",
    "    for K, allHistograms in histograms.items():\n",
    "        fileNameOfHistograms = f\"{prefix}_{type}_hynet_histograms_{K}.pkl\"\n",
    "        filePathOfHistograms = os.path.join(dataDir, fileNameOfHistograms)\n",
    "        with open(filePathOfHistograms, 'wb') as f:\n",
    "            pickle.dump(allHistograms, f)"
   ],
   "id": "2ad37ba8a02c62ce",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T01:03:10.430603Z",
     "start_time": "2024-12-10T01:03:10.428576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Work with data produced during sift feature extraction\n",
    "# TurCoins dataset\n",
    "prefix = \"turcoins\"\n",
    "types = [\"train\", \"test\"]\n",
    "dataDir = \"/Users/lkk/Documents/BOUN CMPE/CMPE 537-Computer Vision/Assignment3/ProducedData/100_64_sklearn\"\n",
    "# HyNet descriptor\n",
    "hynet = HyNet(pretrained=True)\n",
    "# Number of cluster centers\n",
    "Ks = [500, 100, 50]\n",
    "trainedModels = {}"
   ],
   "id": "133f2a22fb70b992",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:47:50.909254Z",
     "start_time": "2024-12-09T20:47:18.005022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for type in types:\n",
    "    # Get image paths and corresponding laf (local affine frames) info\n",
    "    imagePaths, lafs = loadImagePathsAndLAFs(dataDir, prefix, type)\n",
    "\n",
    "    # Get HyNet descriptors of images\n",
    "    descriptors = extractHyNetDescriptors(imagePaths, lafs, hynet)\n",
    "\n",
    "    clusterLabels = {}\n",
    "    histograms = {}\n",
    "    for K in Ks:\n",
    "        # Get cluster labels of HyNet descriptors\n",
    "        performKMeans(trainedModels, K, descriptors, clusterLabels, type)\n",
    "        # Compute bow representation of images using cluster labels\n",
    "        computeHistograms(K, descriptors, clusterLabels, histograms)\n",
    "\n",
    "    # Export data to files\n",
    "    exportToFiles(descriptors, histograms, prefix, type, dataDir)"
   ],
   "id": "d712cf4b369800fb",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Repeat for Caltech\n",
    "    prefix = \"caltech\"\n",
    "    trainedModels = {}\n",
    "    main(prefix, types, dataDir, hynet, Ks, trainedModels)"
   ],
   "id": "e8e5aed17d59ac79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T01:04:20.075137Z",
     "start_time": "2024-12-10T01:03:16.702004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for type in types:\n",
    "    # Get image paths and corresponding laf (local affine frames) info\n",
    "    imagePaths, lafs = loadImagePathsAndLAFs(dataDir, prefix, type)\n",
    "\n",
    "    # Get HyNet descriptors of images\n",
    "    descriptors = extractHyNetDescriptors(imagePaths, lafs, hynet)\n",
    "\n",
    "    clusterLabels = {}\n",
    "    histograms = {}\n",
    "    for K in Ks:\n",
    "        # Get cluster labels of HyNet descriptors\n",
    "        performKMeans(trainedModels, K, descriptors, clusterLabels, type)\n",
    "        # Compute bow representation of images using cluster labels\n",
    "        computeHistograms(K, descriptors, clusterLabels, histograms)\n",
    "\n",
    "    # Export data to files\n",
    "    exportToFiles(descriptors, histograms, prefix, type, dataDir)"
   ],
   "id": "697e20f0803cc333",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "481568f6794bbc2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
